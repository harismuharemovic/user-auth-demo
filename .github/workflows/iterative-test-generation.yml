name: Iterative Test Generation with I/O Validation

on:
  push:
    branches:
      - 'claude/issue-*'

permissions:
  contents: write
  issues: write
  pull-requests: write
  actions: read

jobs:
  extract-metadata:
    runs-on: ubuntu-latest
    outputs:
      issue_number: ${{ steps.extract.outputs.issue_number }}
      jira_key: ${{ steps.extract.outputs.jira_key }}
      attempt_number: ${{ steps.attempt.outputs.attempt_number }}
      should_skip: ${{ steps.check_skip.outputs.should_skip }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check if this is a retry feedback commit
        id: check_skip
        run: |
          COMMIT_MSG=$(git log -1 --pretty=%B)
          echo "Commit message: $COMMIT_MSG"
          
          if echo "$COMMIT_MSG" | grep -q "^test: retry attempt"; then
            echo "should_skip=true" >> $GITHUB_OUTPUT
            echo "⏭️  Skipping - this is a retry feedback commit (will trigger Claude instead)"
          elif echo "$COMMIT_MSG" | grep -q "^chore: remove test file"; then
            echo "should_skip=true" >> $GITHUB_OUTPUT
            echo "⏭️  Skipping - test file cleanup commit (artifact already uploaded)"
          else
            echo "should_skip=false" >> $GITHUB_OUTPUT
            echo "✅ This is a test generation commit - proceeding with validation"
          fi

      - name: Extract issue number from branch
        id: extract
        run: |
          BRANCH_NAME="${{ github.ref_name }}"
          ISSUE_NUMBER=$(echo "$BRANCH_NAME" | grep -oE 'issue-([0-9]+)' | grep -oE '[0-9]+')
          echo "issue_number=$ISSUE_NUMBER" >> $GITHUB_OUTPUT
          echo "Extracted issue number: $ISSUE_NUMBER"
          
          # Get Jira key from issue body
          JIRA_KEY=$(gh issue view "$ISSUE_NUMBER" --json body --jq '.body' | grep -oE 'KAN-[0-9]+' | head -1 || echo "")
          echo "jira_key=$JIRA_KEY" >> $GITHUB_OUTPUT
          echo "Found Jira key: $JIRA_KEY"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Determine attempt number
        id: attempt
        run: |
          ISSUE_NUM="${{ steps.extract.outputs.issue_number }}"
          
          # Count previous attempt comments
          ATTEMPT_COUNT=$(gh issue view "$ISSUE_NUM" --json comments --jq '.comments[].body' | grep -c "Test Attempt" || true)
          if [ -z "$ATTEMPT_COUNT" ] || [ "$ATTEMPT_COUNT" = "" ]; then
            ATTEMPT_COUNT=0
          fi
          ATTEMPT_NUMBER=$((ATTEMPT_COUNT + 1))
          
          echo "attempt_number=$ATTEMPT_NUMBER" >> $GITHUB_OUTPUT
          echo "This is attempt number: $ATTEMPT_NUMBER"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  run-tests-and-validate:
    needs: extract-metadata
    if: needs.extract-metadata.outputs.should_skip != 'true'
    runs-on: ubuntu-latest
    outputs:
      all_passed: ${{ steps.validate.outputs.all_passed }}
      test_file: ${{ steps.find_tests.outputs.test_file }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.ref }}

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm ci

      - name: Find test files
        id: find_tests
        run: |
          # Find new or modified test files
          TEST_FILES=$(git diff --name-only origin/main...HEAD | grep -E '\.(test|spec)\.(ts|tsx|js|jsx)$' || echo "")
          
          if [ -z "$TEST_FILES" ]; then
            echo "No test files found"
            echo "has_tests=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Use first test file found
          TEST_FILE=$(echo "$TEST_FILES" | head -1)
          echo "test_file=$TEST_FILE" >> $GITHUB_OUTPUT
          echo "has_tests=true" >> $GITHUB_OUTPUT
          echo "Found test file: $TEST_FILE"

      - name: Run tests with JSON reporter
        id: run_tests
        continue-on-error: true
        run: |
          # Run Vitest with JSON reporter
          npx vitest run --reporter=json --reporter=verbose --outputFile=/tmp/vitest-results.json 2>&1 | tee /tmp/test_output.txt
          
          # Check exit code
          if [ $? -eq 0 ]; then
            echo "tests_passed=true" >> $GITHUB_OUTPUT
          else
            echo "tests_passed=false" >> $GITHUB_OUTPUT
          fi

      - name: Validate against I/O requirements
        id: validate
        continue-on-error: true
        env:
          OUTPUT_FILE: /tmp/validation-report.json
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ISSUE_NUM="${{ needs.extract-metadata.outputs.issue_number }}"
          
          # Run validation
          if node scripts/validate-test-io.js "$ISSUE_NUM" /tmp/vitest-results.json > /tmp/validation-output.txt; then
            echo "all_passed=true" >> $GITHUB_OUTPUT
            echo "✅ All I/O test cases passed!"
          else
            echo "all_passed=false" >> $GITHUB_OUTPUT
            echo "❌ Some I/O test cases failed"
          fi
          
          # Output validation results
          cat /tmp/validation-output.txt

      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: test-results-attempt-${{ needs.extract-metadata.outputs.attempt_number }}
          path: |
            /tmp/vitest-results.json
            /tmp/validation-report.json
            /tmp/validation-output.txt
            /tmp/test_output.txt

      - name: Upload test file on success
        if: steps.validate.outputs.all_passed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: approved-test-file
          path: ${{ steps.find_tests.outputs.test_file }}

      - name: Remove test file from branch
        if: steps.validate.outputs.all_passed == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          TEST_FILE="${{ steps.find_tests.outputs.test_file }}"
          
          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Remove test file from repo (kept only as artifact)
          if [ -f "$TEST_FILE" ]; then
            git rm "$TEST_FILE"
            git commit -m "chore: remove test file from branch - available as artifact only

            Test file passed validation and has been uploaded as artifact.
            Download locally using: ./scripts/download-approved-test.sh <issue-number>
            
            This prevents test files from being merged into main via PR."
            
            git push origin ${{ github.ref_name }}
            
            echo "✅ Test file removed from branch (available as artifact only)"
          else
            echo "⚠️ Test file not found at $TEST_FILE"
          fi

  handle-success:
    needs: [extract-metadata, run-tests-and-validate]
    if: needs.run-tests-and-validate.outputs.all_passed == 'true'
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          ref: ${{ github.ref }}

      - name: Download test file
        uses: actions/download-artifact@v4
        with:
          name: approved-test-file
          path: /tmp/approved

      - name: Comment on success
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ISSUE_NUM="${{ needs.extract-metadata.outputs.issue_number }}"
          ATTEMPT="${{ needs.extract-metadata.outputs.attempt_number }}"
          TEST_FILE="${{ needs.run-tests-and-validate.outputs.test_file }}"
          
          gh issue comment "$ISSUE_NUM" --body "### ✅ All Test Cases Passed! (Attempt $ATTEMPT)

          All input/output test cases have been validated successfully!
          
          **Test File:** \`$TEST_FILE\`
          **Attempt:** $ATTEMPT of 3
          **Status:** Ready for review
          
          The test file has been saved as an artifact. To retrieve it locally:
          
          \`\`\`bash
          ./scripts/download-approved-test.sh $ISSUE_NUM
          \`\`\`
          
          The test will be placed in the \`unapproved-tests/\` folder for human review."

      - name: Add success label
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ISSUE_NUM="${{ needs.extract-metadata.outputs.issue_number }}"
          gh issue edit "$ISSUE_NUM" --add-label "tests-passed" --add-label "ready-for-review"
          gh issue edit "$ISSUE_NUM" --remove-label "test-in-progress"

  handle-failure:
    needs: [extract-metadata, run-tests-and-validate]
    if: needs.run-tests-and-validate.outputs.all_passed != 'true'
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Download test results
        uses: actions/download-artifact@v4
        with:
          name: test-results-attempt-${{ needs.extract-metadata.outputs.attempt_number }}
          path: /tmp/results

      - name: Check if final attempt
        id: check_final
        run: |
          ATTEMPT="${{ needs.extract-metadata.outputs.attempt_number }}"
          if [ "$ATTEMPT" -ge 3 ]; then
            echo "is_final=true" >> $GITHUB_OUTPUT
            echo "This was the final attempt (3/3)"
          else
            echo "is_final=false" >> $GITHUB_OUTPUT
            echo "Not final attempt yet ($ATTEMPT/3)"
          fi

      # Retry guidance is generated inline in the comment step below

      - name: Post initial failure comment
        if: steps.check_final.outputs.is_final != 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ISSUE_NUM="${{ needs.extract-metadata.outputs.issue_number }}"
          VALIDATION_OUTPUT=$(cat /tmp/results/validation-output.txt)
          ATTEMPT="${{ needs.extract-metadata.outputs.attempt_number }}"
          REMAINING=$((3 - ATTEMPT))
          
          gh issue comment "$ISSUE_NUM" --body "### ❌ Test Attempt $ATTEMPT Failed

          Some test cases did not pass validation. Auto-retry will trigger shortly.
          
          **Validation Results:**
          \`\`\`
          $VALIDATION_OUTPUT
          \`\`\`
          
          **You have $REMAINING attempt(s) remaining.**
          
          📝 Committing RETRY_FEEDBACK.md with detailed failure information..."

      - name: Auto-commit feedback to trigger retry
        if: steps.check_final.outputs.is_final != 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ATTEMPT="${{ needs.extract-metadata.outputs.attempt_number }}"
          VALIDATION_OUTPUT=$(cat /tmp/results/validation-output.txt)
          REMAINING=$((3 - ATTEMPT))
          
          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Checkout the Claude branch
          git fetch origin ${{ github.ref }}
          git checkout ${{ github.ref_name }}
          
          # Create feedback file
          cat > RETRY_FEEDBACK.md << 'EOF'
          # Test Validation Failure Report
          
          **Attempt:** ATTEMPT_NUM/3
          **Remaining Attempts:** REMAINING_NUM
          **Timestamp:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          
          ## Validation Results
          
          ```
          VALIDATION_OUTPUT_PLACEHOLDER
          ```
          
          ## Instructions for AI Test Generator
          
          The tests you generated do not match the expected input/output specifications in the GitHub issue.
          
          **Critical Requirements:**
          1. Each test case must use the EXACT expected output from the I/O table
          2. Test naming must follow: `describe('methodName - TC-XXX', ...)`
          3. Review the validation errors above - they show what your tests returned vs. what was expected
          4. DO NOT change expected values based on code analysis - use ONLY the specified values
          
          **What to fix:**
          - Review each failed TC-XXX case above
          - Update the assertions in your test file to match the Expected Output column
          - Ensure test descriptions match the pattern for validation to work
          
          Please regenerate the test file with corrections and push your changes.
          EOF
          
          # Replace placeholders (using perl for better multiline handling)
          perl -i -pe "s/ATTEMPT_NUM/$ATTEMPT/g" RETRY_FEEDBACK.md
          perl -i -pe "s/REMAINING_NUM/$REMAINING/g" RETRY_FEEDBACK.md
          
          # For validation output, write it separately to avoid escaping issues
          {
            echo "# Test Validation Failure Report"
            echo ""
            echo "**Attempt:** $ATTEMPT/3"
            echo "**Remaining Attempts:** $REMAINING"
            echo "**Timestamp:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
            echo ""
            echo "## Validation Results"
            echo ""
            echo '```'
            cat /tmp/results/validation-output.txt
            echo '```'
            echo ""
            echo "## Instructions for AI Test Generator"
            echo ""
            echo "The tests you generated do not match the expected input/output specifications in the GitHub issue."
            echo ""
            echo "**Critical Requirements:**"
            echo "1. Each test case must use the EXACT expected output from the I/O table"
            echo "2. Test naming must follow: \`describe('methodName - TC-XXX', ...)\`"
            echo "3. Review the validation errors above - they show what your tests returned vs. what was expected"
            echo "4. DO NOT change expected values based on code analysis - use ONLY the specified values"
            echo ""
            echo "**What to fix:**"
            echo "- Review each failed TC-XXX case above"
            echo "- Update the assertions in your test file to match the Expected Output column"
            echo "- Ensure test descriptions match the pattern for validation to work"
            echo ""
            echo "Please regenerate the test file with corrections and push your changes."
          } > RETRY_FEEDBACK.md
          
          # Commit and push with @claude mention in commit message
          ISSUE_NUM="${{ needs.extract-metadata.outputs.issue_number }}"
          
          git add RETRY_FEEDBACK.md
          git commit -m "test: retry attempt $ATTEMPT - validation feedback" \
                     -m "@claude - Tests failed validation. Please:" \
                     -m "1. Read RETRY_FEEDBACK.md for failure details" \
                     -m "2. Update test file to fix assertions" \
                     -m "3. Use exact expected outputs from issue #$ISSUE_NUM I/O table" \
                     -m "4. Commit your changes" \
                     -m "" \
                     -m "Attempt: $ATTEMPT/3" \
                     -m "Remaining: $REMAINING attempts"
          
          git push origin ${{ github.ref_name }}
          
          echo "✅ Feedback committed with @claude mention"

      - name: Trigger Claude for retry via label manipulation
        if: steps.check_final.outputs.is_final != 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ISSUE_NUM="${{ needs.extract-metadata.outputs.issue_number }}"
          ATTEMPT="${{ needs.extract-metadata.outputs.attempt_number }}"
          REMAINING=$((3 - ATTEMPT))
          TEST_FILE="${{ needs.run-tests-and-validate.outputs.test_file }}"
          
          # Post informational comment (not @claude, just info)
          NEXT_ATTEMPT=$((ATTEMPT + 1))
          gh issue comment "$ISSUE_NUM" --body "### 🔄 Retry Attempt $NEXT_ATTEMPT/3

          Test Validation Failed - Detailed feedback in \`RETRY_FEEDBACK.md\`
          
          Failed Test File: \`$TEST_FILE\`
          Remaining Attempts: $REMAINING
          
          🤖 Automatically triggering Claude to fix the tests...
          
          Task for Claude: Update the test assertions in \`$TEST_FILE\` to match the EXACT expected outputs from the I/O specification table in this issue. Review \`RETRY_FEEDBACK.md\` for details on which test cases failed and why."
          
          # Add retry label to trigger Claude re-engagement
          gh issue edit "$ISSUE_NUM" --add-label "test-retry-needed"
          
          # Wait a moment for label to be processed
          sleep 2
          
          # Post @claude mention AFTER label
          NEXT_ATTEMPT=$((ATTEMPT + 1))
          gh issue comment "$ISSUE_NUM" --body "@claude The tests failed validation. Please:

          1. Read \`RETRY_FEEDBACK.md\` in the repo root to see which test cases failed
          2. Update \`$TEST_FILE\` to fix the failing assertions
          3. Use the EXACT expected outputs from the I/O table in this issue
          4. Keep normal test pattern: \`const result = fn(...); expect(result).toBe(value);\`
          5. Commit your changes

          This is retry attempt $NEXT_ATTEMPT/3. You have $REMAINING more attempt(s) after this."
          
          echo "✅ Triggered Claude retry via label + comment"

      - name: Handle final failure
        if: steps.check_final.outputs.is_final == 'true'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ATLASSIAN_USER_EMAIL: ${{ secrets.ATLASSIAN_USER_EMAIL }}
          ATLASSIAN_API_TOKEN: ${{ secrets.ATLASSIAN_API_TOKEN }}
          ATLASSIAN_DOMAIN: ${{ secrets.ATLASSIAN_DOMAIN }}
        run: |
          ISSUE_NUM="${{ needs.extract-metadata.outputs.issue_number }}"
          JIRA_KEY="${{ needs.extract-metadata.outputs.jira_key }}"
          VALIDATION_OUTPUT=$(cat /tmp/results/validation-output.txt)
          
          # Extract first failed test case for summary
          FAILED_TC=$(echo "$VALIDATION_OUTPUT" | grep "❌ TC-" | head -1 | grep -oE "TC-[0-9]+" || echo "Multiple")
          
          # Comment on GitHub issue
          gh issue comment "$ISSUE_NUM" --body "### ❌ Test Generation Failed After 3 Attempts
          
          Unable to generate passing tests after 3 attempts.
          
          **Final Validation Results:**
          \`\`\`
          $VALIDATION_OUTPUT
          \`\`\`
          
          **Next Steps:**
          1. Review the Jira ticket for accuracy
          2. Check if expected outputs are correct
          3. Verify the method implementation handles all cases
          4. Consider manual test creation or code fixes
          
          The Jira ticket has been updated with this failure information."
          
          # Add failure labels
          gh issue edit "$ISSUE_NUM" --add-label "test-generation-failed" --remove-label "test-in-progress"
          gh issue close "$ISSUE_NUM"
          
          # Update Jira ticket
          if [ ! -z "$JIRA_KEY" ] && [ ! -z "$ATLASSIAN_API_TOKEN" ]; then
            echo "Updating Jira ticket $JIRA_KEY..."
            ./scripts/update-jira-test-failure.sh "$JIRA_KEY" "$FAILED_TC" "$ISSUE_NUM" "$VALIDATION_OUTPUT" || echo "Failed to update Jira"
          fi

